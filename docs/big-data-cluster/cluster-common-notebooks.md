---
title: Jupyter Notebooks と Azure Data Studio を使用してビッグ データ クラスター (BDC) を操作する一般的なシナリオ
titleSuffix: SQL Server Big Data Clusters
description: SQL Server 2019 ビッグ データ クラスター上で、Jupyter Notebooks と Azure Data Studio を使用して BDC を操作する一般的なシナリオ。
author: cloudmelon
ms.author: melqin
ms.reviewer: mikeray
ms.metadata: seo-lt-2019
ms.date: 09/22/2020
ms.topic: conceptual
ms.prod: sql
ms.technology: big-data-cluster
ms.openlocfilehash: 99e62be597e4ce08d38db199116f1bd4d5ab33f6
ms.sourcegitcommit: 29a2be59c56f8a4b630af47760ef38d2bf56a3eb
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 10/22/2020
ms.locfileid: "92378424"
---
# <a name="common-notebooks-for-sql-server-big-data-clusters"></a>SQL Server ビッグ データ クラスター用の一般的なノートブック

この記事には、SQL Server ビッグ データ クラスター用のノートブックが一覧表示されています。 実行可能ノートブック (.ipynb) は、ビッグ データ クラスターの一般的なシナリオを示すのに役立つように SQL Server 2019 向けに設計されています。

各ノートブックは、独自の依存関係を確認するように設計されています。 **[Run all cells]\(すべてのセルを実行\)** オプションは、正常に完了するか、例外が発生し、欠落している依存関係を解決するための別のノートブックにハイパーリンクされた " *ヒント* " が示されます。 後続のノートブックへのヒント ハイパーリンクに従い、 **[Run all cells]\(すべてのセルを実行\)** をクリックし、成功した場合は元のノートブックに戻り、 **[Run all cells]\(すべてのセルを実行\)** をクリックします。

すべての依存関係がインストールされていても、 **[Run all cells]\(すべてのセルを実行\)** に失敗した場合は、各ノートブックを使用して結果を分析し、可能であれば、問題の解決にさらに役立つように別のノートブックにハイパーリンクされたヒントを生成します。

## <a name="gathering-logs-from-big-data-cluster-bdc"></a>ビッグ データ クラスター (BDC) からのログの収集

このセクションのノートブックは、クラスターのログインやログアウトなど、他のノートブックの前提条件として使用されます。

|名前 |説明 |
|---|---|
|SOP005 - az login|az コマンド ライン インターフェイスを使って Azure にログインします。 |
|SOP006 - az logout|az コマンド ライン インターフェイスを使って Azure からログアウトします。|
|SOP007 - バージョン情報 (azdata、bdc、kubernetes)|バージョン管理情報に関するノートブックで使用されるヘルパー関数を定義します。|
|SOP011 - kubernetes 構成のコンテキストを設定する|使用する kubernetes 構成を設定します。 |
|SOP028 - azdata login|azdata コマンド ライン インターフェイスを使ってビッグ データ クラスターにログインします。 |
|SOP033 - azdata logout|azdata コマンド ライン インターフェイスを使ってビッグ データ クラスターからログアウトします。 |
|SOP034 - BDC が正常になるまで待機する|ビッグ データ クラスターが正常になるか、指定されたタイムアウト時間が経過するまでブロックします。min_pod_count パラメーターは、少なくともこの数のポッドがクラスターに存在するようになるまで、正常性チェックに合格しないことを示します。 この制限を超える既存のポッドがすべて異常な場合、クラスターは正常ではありません。|
|OPR001 - app-deploy を作成する|このノートブックを使用して、ビッグ データ クラスターにアプリを作成します。 |
|OPR002 - app-deploy を実行する|このノートブックを使用して、ビッグ データ クラスターでアプリを実行します。 |
|OPR003 - cronjob を作成する|このノートブックを使用して、ビッグ データ クラスターに cronjob を作成します。 |
|OPR004 - cronjob を中断する|このノートブックを使用して、ビッグ データ クラスターで cronjob を中断します。 |
|OPR005 - cronjob を再開する|このノートブックを使用して、ビッグ データ クラスターで cronjob を再開します。 |
|OPR006 - cronjob を削除する|このノートブックを使用して、ビッグ データ クラスターで cronjob を削除します。 |
|OPR007 - app-deploy を削除する|このノートブックを使用して、ビッグ データ クラスターでアプリを削除します。 |
|OPR100 - ノートブックをデプロイしてスケジュールを設定する|このノートブックを使用して、App-Deploy ポッドにノートブックの一覧をデプロイし、Kubernetes CronJob を使用してスケジュールに従って App-Deploy を実行し、結果を表示するための Grafana ダッシュボードをインストールします。|
|OPR600 - インフラストラクチャを監視する (Kubernetes)|このノートブックを使用して、インフラストラクチャを監視します。|
|OPR700 - App-Deploy アプリケーション用の Grafana ダッシュボードを作成する|このノートブックを使用して Grafana でダッシュボードを作成して App-Deploy の結果を監視し、カナリアまたはアプリケーションが失敗し始めた場合にアラートを生成します。|
|OPR900 - app-deploy の実行のトラブルシューティングを行う|このノートブックを使用して、(kubectl exec を使って) コンテナーで直接 app-deploy スクリプトを実行します。 これにより、特にスクリプトの起動段階での問題に関連する、問題をトラブルシューティングするためのより多くのデバッグ情報が得られます。|
|OPR901 - cronjob のトラブルシューティングを行う|このノートブックを使用し、(kubectl exec を使って) コンテナーで直接 cronjob スクリプトを実行します。 これにより、問題をトラブルシューティングするためのより多くのデバッグ情報が得られます。|


## <a name="analyze-logs-from-big-data-clusters-bdc"></a>ビッグ データ クラスター (BDC) からのログを分析する

SQL Server ビッグ データ クラスターのシナリオを示す一連のサンプル ノートブック。

|名前 |説明 |
|---|---|
|SAM001a - SQL Server マスター プールから記憶域プールのクエリを実行する (1/3) - サンプル データを読み込む|この 3 部構成のチュートリアルでは、azdata を使用して記憶域プール (HDFS) にデータを読み込み、それを (Spark を使用して) Parquet に変換し、第 3 部でマスター プール (SQL Server) を使用してデータのクエリを実行します。 |
|SAM001b - SQL Server マスター プールから記憶域プールのクエリを実行する (2/3) - データを parquet に変換する|3 部構成のチュートリアルのこの第 2 部では、Spark を使用して .csv ファイルを parquet ファイルに変換します。|
|SAM001c - SQL Server マスター プールから記憶域プールのクエリを実行する (3/3) - SQL Server から HDFS のクエリを実行する|記憶域プールのチュートリアルのこの第 3 部では、ビッグ データ クラスターで HDFS データを指す外部テーブルを作成し、このデータをマスター インスタンスの価値の高いデータと結合する方法について学習します。|
|SAM002 - 記憶域プール (2/2) - HDFS のクエリを実行する|記憶域プールのチュートリアルのこの第 2 部では、ビッグ データ クラスターで HDFS データを指す外部テーブルを作成し、このデータをマスター インスタンスの価値の高いデータと結合する方法について学習します|
|SAM003 - データ プールの例|このチュートリアルでは、データ プール ソースと外部テーブルをデータ プールに作成してから、データ プール テーブルにデータを挿入して、データ プール テーブル間でデータを読み込む方法について学習します。 データ プール テーブル内のデータを他のデータ プール テーブルと結合し、テーブルの切り捨てとクリーンアップも行います。 |
|SAM004 - MongoDB からのデータを仮想化する|MongoDB の外部データ ソースのデータに対してクエリを実行するには、外部テーブルを作成してその外部データを参照する必要があります。 このセクションでは、これらの外部テーブルを作成するサンプル コードを示します。|
|SAM005 - Oracle からのデータを仮想化する|Oracle の外部データ ソースのデータに対してクエリを実行するには、外部テーブルを作成してその外部データを参照する必要があります。 このセクションでは、これらの外部テーブルを作成するサンプル コードを示します。|
|SAM006 - SQL Server からのデータを仮想化する|別の SQL Server データ ソースからのデータに対して仮想的にクエリを実行するには、外部テーブルを作成してその外部データを参照する必要があります。 このセクションでは、これらの外部テーブルを作成するサンプル コードを示します。|
|SAM007 - Teradata からのデータを仮想化する|Teradata の外部データ ソースのデータに対してクエリを実行するには、外部テーブルを作成してその外部データを参照する必要があります。 このセクションでは、これらの外部テーブルを作成するサンプル コードを示します。|
|SAM008 - azdata を使用した Spark|Spark セッションを操作するための azdata および kubectl コマンド。|
|SAM009 - azdata を使用した HDFS|HDFS を操作するための azdata および kubectl コマンド。|
|SAM010 - azdata を使用したアプリ|App Deploy を操作するための azdata および kubectl コマンド。 |

## <a name="next-steps"></a>次のステップ

[!INCLUDE[big-data-clusters-2019](../includes/ssbigdataclusters-ss-nover.md)]の詳細については、「[[!INCLUDE[big-data-clusters-2019](../includes/ssbigdataclusters-ver15.md)]とは](big-data-cluster-overview.md)」を参照してください。
